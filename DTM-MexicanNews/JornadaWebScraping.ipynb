{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05ac769",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7260aa0-5ab9-43d1-9603-8d9c9c73c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webscraping tools\n",
    "from scrapy import Selector\n",
    "import requests\n",
    "\n",
    "# Data manipulation tools\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Flow control tools\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af8c2c",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "446f1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_range(\n",
    "                start_date = datetime.today().strftime( '%Y/%m/%d' ), \n",
    "                end_date = datetime.today().strftime( '%Y/%m/%d' )\n",
    "                ):\n",
    "    '''\n",
    "    This function will give you a list of dates with the format yyyy/mm/dd.\n",
    "    \n",
    "    It requires as input:\n",
    "        - start_date : this is the first date of the range. Please enter a sring in format \"mm-dd-yyyy\"\n",
    "        - end_date   : this is the last date of the range. Please enter a sring in format \"mm-dd-yyyy\"\n",
    "        \n",
    "    If no input is given, it will give you only today's date. This also allows you to give only start_date\n",
    "    as input and get the date range up to today's date.\n",
    "    '''\n",
    "    # Create a date range\n",
    "    dates = pd.date_range( start = start_date, end = end_date )\n",
    "    \n",
    "    # Mutate to a list of strings with the format yyyy/mm/dd\n",
    "    dates = dates.format( formatter = lambda date: date.strftime( '%Y/%m/%d' ) )\n",
    "    \n",
    "    # Return the list of dates\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "870f8302-8e07-4c04-9e30-5542f2dcd227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def editorial_scraper(date = datetime.today().strftime( '%Y/%m/%d' )):\n",
    "    \n",
    "    '''This scraper will give you the \"editorial\" section from La Jornada'''\n",
    "    \n",
    "    # Set section url\n",
    "    url = 'https://www.jornada.com.mx/' + date + '/edito'\n",
    "    \n",
    "    # Headers of the request\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0' }\n",
    "    \n",
    "    # Make a request on the main page\n",
    "    html = requests.get( url, headers = headers ).content\n",
    "    selector = Selector( text = html )\n",
    "\n",
    "    # Create a dataframe row\n",
    "    row = {\n",
    "        'date' : date,\n",
    "        'url' : url,\n",
    "        'section' : 'editorial',\n",
    "        'title' : ''.join(selector.xpath( '//article//div[@class=\"cabeza\"]//text()' ).extract()),\n",
    "        'author' : 'Editorial',\n",
    "        'raw_text' :''.join(selector.xpath( '//article//div[@id=\"article-text\"]//div//text()' ).extract())\n",
    "    }  \n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "706dd0b4-b178-49a2-8367-d31182706e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correo_scraper(date = datetime.today().strftime( '%Y/%m/%d' )):\n",
    "    \n",
    "    '''This scraper will give you the \"correo ilustrado\" section from La Jornada'''\n",
    "    \n",
    "    # Set section url\n",
    "    url = 'https://www.jornada.com.mx/' + date + '/correo'\n",
    "\n",
    "    # Headers of the request\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0' }\n",
    "\n",
    "    # Make a request on the main page\n",
    "    html = requests.get( url, headers = headers ).content\n",
    "    selector = Selector( text = html )\n",
    "\n",
    "    # Create a dataframe row\n",
    "    row = {\n",
    "        'date' : date,\n",
    "        'url' : url,\n",
    "        'section' : 'correo_ilustrado',\n",
    "        'title' : ''.join(selector.xpath( '//article//div[@class=\"cabeza\"]//text()' ).extract()),\n",
    "        'author' : 'Correo Ilustrado',\n",
    "        'raw_text' :''.join(selector.xpath( '//article//div[@id=\"article-text\"]//div//text()' ).extract())\n",
    "    }\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a4add7d-e2af-40f9-83e6-5af5f3dfc546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(date, article):\n",
    "    \n",
    "    '''This scraper will return the article and its matadata from an specific url'''\n",
    "    \n",
    "    # Make article url\n",
    "    article_url = 'https://www.jornada.com.mx/' + date + '/' + article\n",
    "    \n",
    "    # Headers of the request\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0' }\n",
    "    \n",
    "    get_request  = requests.get( article_url, headers = headers )\n",
    "    html = get_request.content\n",
    "    selector = Selector( text = html )\n",
    "\n",
    "    # Create a dataframe row\n",
    "    row = {\n",
    "        'date' : date,\n",
    "        'url' : article_url,\n",
    "        'section' : article.split('/')[0],\n",
    "        'title' : ''.join(selector.xpath( '//article//div[@class=\"cabeza\"]//text()' ).extract()),\n",
    "        'author' : ''.join(selector.xpath( '//article//div[@itemprop=\"author\"]//text()' ).extract()),\n",
    "        'raw_text' :''.join(selector.xpath( '//article//div[@id=\"article-text\"]//div//text()' ).extract())\n",
    "        }\n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47d4100b-b1b2-4605-ba76-e00cbf34aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_scraper(date = datetime.today().strftime( '%Y/%m/%d' )):\n",
    "    \n",
    "    '''This scraper will give you all articles in the \"opinion\" section from La Jornada'''\n",
    "    \n",
    "    # Set section url\n",
    "    url = 'https://www.jornada.com.mx/' + date + '/opinion'\n",
    "    \n",
    "    # Headers of the request\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0' }\n",
    "    \n",
    "    # Make a request on the main page\n",
    "    html = requests.get( url, headers = headers ).content\n",
    "    selector = Selector( text = html )\n",
    "\n",
    "    # Get articles urls\n",
    "    articles = list(set(selector.xpath('//div[@id=\"section-cont\"]/div[contains(@class,\"item\")]//a/@href').extract()))\n",
    "\n",
    "    # Get articles into a dataframe\n",
    "    articles_df = [get_article(date,article) for article in articles]\n",
    "\n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3af1072b-17d6-4326-9585-02e55f57f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def economia_scraper(date = datetime.today().strftime( '%Y/%m/%d' )):\n",
    "    \n",
    "    '''This scraper will give you all articles in the \"economia\" section from La Jornada'''\n",
    "    \n",
    "    # Set section url\n",
    "    url = 'https://www.jornada.com.mx/' + date + '/economia'\n",
    "    \n",
    "    # Headers of the request\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0' }\n",
    "\n",
    "    # Make a request on the main page\n",
    "    html = requests.get( url, headers = headers ).content\n",
    "    selector = Selector( text = html )\n",
    "\n",
    "    # Get articles urls\n",
    "    articles = list(set(selector.xpath('//div[@id=\"section-cont\"]/div[contains(@class,\"item\")]//a/@href').extract()))\n",
    "\n",
    "    # Get articles into a dataframe\n",
    "    articles_df = [get_article(date,article) for article in articles]\n",
    "\n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e8e0000-b680-475b-8a5f-520b9124dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def politica_scraper(date = datetime.today().strftime( '%Y/%m/%d' )):\n",
    "    \n",
    "    '''This scraper will give you all articles in the \"politica\" section from La Jornada'''\n",
    "    \n",
    "    # Set section url\n",
    "    url = 'https://www.jornada.com.mx/' + date + '/politica'\n",
    "    \n",
    "    # Headers of the request\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0' }\n",
    "    \n",
    "    # Make a request on the main page\n",
    "    html = requests.get( url, headers = headers ).content\n",
    "    selector = Selector( text = html )\n",
    "\n",
    "    # Get articles urls\n",
    "    articles = list(set(selector.xpath('//div[@id=\"section-cont\"]/div[contains(@class,\"item\")]//a/@href').extract()))\n",
    "\n",
    "    # Get articles into a dataframe\n",
    "    articles_df = [get_article(date,article) for article in articles]\n",
    "\n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5e24928-2ae4-4122-8625-688af9e5de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estados_scraper(date = datetime.today().strftime( '%Y/%m/%d' )):\n",
    "    \n",
    "    '''This scraper will give you all articles in the \"estados\" section from La Jornada'''\n",
    "    \n",
    "    # Set section url\n",
    "    url = 'https://www.jornada.com.mx/' + date + '/estados'\n",
    "    \n",
    "    # Headers of the request\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0' }\n",
    "\n",
    "    # Make a request on the main page\n",
    "    html = requests.get( url, headers = headers ).content\n",
    "    selector = Selector( text = html )\n",
    "\n",
    "    # Get articles urls\n",
    "    articles = list(set(selector.xpath('//div[@id=\"section-cont\"]/div[contains(@class,\"item\")]//a/@href').extract()))\n",
    "\n",
    "    # Get articles into a dataframe\n",
    "    articles_df = [get_article(date,article) for article in articles]\n",
    "\n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d76d40e6-4c93-4877-b701-d5dee991eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_scraper(date = datetime.today().strftime( '%Y/%m/%d' )):\n",
    "    \n",
    "    '''This scraper will give you all articles in the \"capital\" section from La Jornada'''\n",
    "    \n",
    "    # Set section url\n",
    "    url = 'https://www.jornada.com.mx/' + date + '/capital'\n",
    "    \n",
    "    # Headers of the request\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0' }\n",
    "\n",
    "    # Make a request on the main page\n",
    "    html = requests.get( url, headers = headers ).content\n",
    "    selector = Selector( text = html )\n",
    "\n",
    "    # Get articles urls\n",
    "    articles = list(set(selector.xpath('//div[@id=\"section-cont\"]/div[contains(@class,\"item\")]//a/@href').extract()))\n",
    "\n",
    "    # Get articles into a dataframe\n",
    "    articles_df = [get_article(date,article) for article in articles]\n",
    "    \n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0def628a-8719-4156-9653-88add8b3ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cultura_scraper(date = datetime.today().strftime( '%Y/%m/%d' )):\n",
    "    \n",
    "    '''This scraper will give you all articles in the \"cultura\" section from La Jornada'''\n",
    "    \n",
    "    # Set section url\n",
    "    url = 'https://www.jornada.com.mx/' + date + '/cultura'\n",
    "    \n",
    "    # Headers of the request\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0' }\n",
    "\n",
    "    # Make a request on the main page\n",
    "    html = requests.get( url, headers = headers ).content\n",
    "    selector = Selector( text = html )\n",
    "\n",
    "    # Get articles urls\n",
    "    articles = list(set(selector.xpath('//div[@id=\"section-cont\"]/div[contains(@class,\"item\")]//a/@href').extract()))\n",
    "\n",
    "    # Get articles into a dataframe\n",
    "    articles_df = [get_article(date,article) for article in articles]\n",
    "    \n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47dc465b-1351-4c82-836a-50cb84d278f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def espectaculos_scraper(date = datetime.today().strftime( '%Y/%m/%d' )):\n",
    "    \n",
    "    '''This scraper will give you all articles in the \"espectaculos\" section from La Jornada'''\n",
    "    \n",
    "    # Set section url\n",
    "    url = 'https://www.jornada.com.mx/' + date + '/espectaculos'\n",
    "    \n",
    "    # Headers of the request\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0' }\n",
    "\n",
    "    # Make a request on the main page\n",
    "    html = requests.get( url, headers = headers ).content\n",
    "    selector = Selector( text = html )\n",
    "\n",
    "    # Get articles urls\n",
    "    articles = list(set(selector.xpath('//div[@id=\"section-cont\"]/div[contains(@class,\"item\")]//a/@href').extract()))\n",
    "\n",
    "    # Get articles into a dataframe\n",
    "    articles_df = pd.concat([get_article(date,article) for article in articles])\n",
    "\n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3f059fb-82ec-461d-a9d0-978fbdc9e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deportes_scraper(date = datetime.today().strftime( '%Y/%m/%d' )):\n",
    "    \n",
    "    '''This scraper will give you all articles in the \"deportes\" section from La Jornada'''\n",
    "    \n",
    "    # Set section url\n",
    "    url = 'https://www.jornada.com.mx/' + date + '/deportes'\n",
    "    \n",
    "    # Headers of the request\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0' }\n",
    "\n",
    "    # Make a request on the main page\n",
    "    html = requests.get( url, headers = headers ).content\n",
    "    selector = Selector( text = html )\n",
    "\n",
    "    # Get articles urls\n",
    "    articles = list(set(selector.xpath('//div[@id=\"section-cont\"]/div[contains(@class,\"item\")]//a/@href').extract()))\n",
    "\n",
    "    # Get articles into a dataframe\n",
    "    articles_df = [get_article(date,article) for article in articles]\n",
    "\n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf7f1cc",
   "metadata": {},
   "source": [
    "### La Jornada Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c530c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(date = datetime.today().strftime( '%Y/%m/%d' )):\n",
    "    '''\n",
    "    This function will give you the new published in La Jornada on the specified date\n",
    "    '''\n",
    "    # This is the base url\n",
    "    jornada_url = 'https://www.jornada.com.mx' \n",
    "    url = jornada_url + '/' + date\n",
    "\n",
    "    # Headers of the request\n",
    "    headers = { 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0' }\n",
    "    \n",
    "    # Make a request on the main page\n",
    "    get_request  = requests.get( url, headers = headers )\n",
    "\n",
    "    # If the request is successful\n",
    "    if get_request.status_code < 400:\n",
    "\n",
    "        # Get html content\n",
    "        html = get_request.content\n",
    "        # Get selectors\n",
    "        selector = Selector( text = html )\n",
    "        # Get the urls for each section\n",
    "        sections = ['edito','correo','opinion','politica','economia','estados','capital','cultura','espectaculos','deportes']\n",
    "        # Create an empty dataframe\n",
    "        dataframe = pd.DataFrame(\n",
    "                                {\n",
    "                                    'date' : [],\n",
    "                                    'id' : [],\n",
    "                                    'section' : [],\n",
    "                                    'title' : [],\n",
    "                                    'author' : [],\n",
    "                                    'raw_text' : []\n",
    "                                }\n",
    "                                )\n",
    "\n",
    "        # Iterate over each section\n",
    "        for section in tqdm(sections):\n",
    "            url = jornada_url + '/' + date +  '/' + section\n",
    "            \n",
    "            # Do a request on the main page\n",
    "            html = requests.get( url, headers = headers ).content\n",
    "            selector = Selector( text = html )\n",
    "\n",
    "            # Editorial structure\n",
    "            if 'edito' in section:\n",
    "\n",
    "                # Create a dataframe row\n",
    "                row = pd.DataFrame(\n",
    "                    {\n",
    "                        'date' : date,\n",
    "                        'id' : url.replace(jornada_url,''),\n",
    "                        'section' : 'editorial',\n",
    "                        'title' : ''.join(selector.xpath( '//article//div[@class=\"cabeza\"]//text()' ).extract()),\n",
    "                        'author' : 'Editorial',\n",
    "                        'raw_text' :''.join(selector.xpath( '//article//div[@id=\"article-text\"]//div//text()' ).extract())\n",
    "                    },\n",
    "                    index =[0]\n",
    "                    )\n",
    "\n",
    "                # Append to the dataframe\n",
    "                dataframe = pd.concat([dataframe, row])\n",
    "\n",
    "            # Correo Ilustrado structure\n",
    "            elif 'correo' in section:\n",
    "\n",
    "                # Create a dataframe row\n",
    "                row = pd.DataFrame(\n",
    "                    {\n",
    "                        'date' : date,\n",
    "                        'id' : url.replace(jornada_url,''),\n",
    "                        'section' : 'correo_ilustrado',\n",
    "                        'title' : ''.join(selector.xpath( '//article//div[@class=\"cabeza\"]//text()' ).extract()),\n",
    "                        'author' : 'Correo Ilustrado',\n",
    "                        'raw_text' :''.join(selector.xpath( '//article//div[@id=\"article-text\"]//div//text()' ).extract())\n",
    "                    },\n",
    "                    index =[0]\n",
    "                    )\n",
    "\n",
    "                # Append to the dataframe\n",
    "                dataframe = pd.concat([dataframe, row])\n",
    "\n",
    "            # Cartones structure\n",
    "            elif 'cartones' in section:\n",
    "                # Just pass since this proyect is not about images(FOR NOW)\n",
    "                pass\n",
    "\n",
    "            # News articles structure\n",
    "            else:\n",
    "                # Get articles urls\n",
    "                articles = list(set(selector.xpath('//div[@id=\"section-cont\"]/div[contains(@class,\"item\")]//a/@href').extract()))\n",
    "                # For each article\n",
    "                for article in articles:\n",
    "                    article_url = 'https://www.jornada.com.mx/' + date + '/' + article\n",
    "                    # Check if available\n",
    "                    if 'remove_article' in article_url:\n",
    "                                removed_articles.append(article_url)\n",
    "                    else:\n",
    "                        # Maker request\n",
    "                        get_request  = requests.get( article_url, headers = headers )\n",
    "                        html = get_request.content\n",
    "                        selector = Selector( text = html )\n",
    "                        \n",
    "                        # Create a dataframe row\n",
    "                        row = pd.DataFrame(\n",
    "                            {\n",
    "                                'date' : date,\n",
    "                                'id' : article_url.replace(jornada_url,''),\n",
    "                                'section' : section.split('/')[-1],\n",
    "                                'title' : ''.join(selector.xpath( '//article//div[@class=\"cabeza\"]//text()' ).extract()),\n",
    "                                'author' : ''.join(selector.xpath( '//article//div[@itemprop=\"author\"]//text()' ).extract()),\n",
    "                                'raw_text' :''.join(selector.xpath( '//article//div[@id=\"article-text\"]//div//text()' ).extract())\n",
    "                            },\n",
    "                            index =[0]\n",
    "                            )\n",
    "\n",
    "                        # Append to the  dataframe\n",
    "                        dataframe = pd.concat([dataframe, row])\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e8b86",
   "metadata": {},
   "source": [
    "### Load to a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5d322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▍                              | 26/356 [10:59:13<161:25:52, 1761.07s/it]"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Establish SQL connection\n",
    "sqliteConnection = sqlite3.connect('./data/dtm_master_project.db')\n",
    "cursor = sqliteConnection.cursor()\n",
    "\n",
    "# Headers of the request\n",
    "headers = { 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0' }\n",
    "#Get daily news\n",
    "dates = date_range('01-01-2021')\n",
    "for date in tqdm(dates):\n",
    "    try:\n",
    "        news = jornada_allcrawler(date)\n",
    "        # Push to the database\n",
    "        news.to_sql('raw_news',sqliteConnection, if_exists='append',index = False)\n",
    "    except:\n",
    "        pass\n",
    "# Close connection\n",
    "sqliteConnection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
